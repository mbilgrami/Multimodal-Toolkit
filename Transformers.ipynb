{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbXtGYYhwi4OkvB35aiRM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbilgrami/Multimodal-Toolkit/blob/master/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers**\n",
        "\n",
        "Links:\n",
        "* https://www.youtube.com/watch?v=ISNdQcPhsts&ab_channel=UmarJamil\n",
        "* https://www.jeremyjordan.me/transformer-architecture/\n"
      ],
      "metadata": {
        "id": "XP2lFqnjz_Kt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qjRkVJVmuGc",
        "outputId": "8fda4455-f938-4f26-8884-e753a9f33497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "em0RAOzoEuVf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Architecture**"
      ],
      "metadata": {
        "id": "hVZ1JPu1Nnhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png)"
      ],
      "metadata": {
        "id": "AdXQYL6KNQ32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.google.com.au/url?sa=i&url=https%3A%2F%2Fmachinelearningmastery.com%2Fthe-transformer-model%2F&psig=AOvVaw0XarZVK2XDMc08z38MRZkl&ust=1711332315602000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCLimy-jni4UDFQAAAAAdAAAAABAE)"
      ],
      "metadata": {
        "id": "bv6sWQvxNWGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can get from public github folders: with this command:\n",
        "\"\"\"\n",
        "![](https://raw.githubusercontent.com/ivonnics/Machine-Learning/master/Tabla%20Confussion%20Matrix.png)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eOb7x5EPwIP0",
        "outputId": "8ab659db-de2d-41ce-faf6-be8edc8f2bee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n![](https://raw.githubusercontent.com/ivonnics/Machine-Learning/master/Tabla%20Confussion%20Matrix.png)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embeddings**"
      ],
      "metadata": {
        "id": "SRiCegjiX-Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings explained\n",
        "\n",
        "The embeddings class is a matrix of dimension (vocabulary_size, d_model). It's kind of like a look up index for each word, which is why the number of rows is equivalent to vocabulary_size.\n",
        "\n",
        "The d_model represents the dimensions that each word is represented with. So d_model = 3 means that each word is expected to have 3 dimensions.\n",
        "\n",
        "The Embedding tensor is initialised randomly, and then the weights are updated over time during training. It is essentially just the weight component of an nn.Linear.\n",
        "\n",
        "&nbsp;\n",
        "#### Example\n",
        "Consider an embedding matrix of shape (10 , 3). This means there are 10 rows (vocabulary is 10) and 3 dimensions for each word - so we get 10 rows of a 1x3 matrix. During training, lets say we feed in a two word sentence, represented by [1,2,3] and [4,5,6]. This will mean that the input word tensor will be [[1,2,3], [4,5,6]].\n",
        "\n",
        "The output tensor will be a 2x3x3 tensor. This means for each of the two words, there will be a 3x(1x3) tensor, which will be created.\n",
        "\n",
        "Remember how the tensor was a look up? The word [1,2,3] will have weights for index 1, index 2 and index 3 of the Embedding matrix. So that means, the second row of 1x3, the third row of 1x3 and the 4th row of 1x3. This makes it a 2x3x3 tensor.\n",
        "\n",
        "I think dim_model represents the relationship that multiple words can have with one another, so for a dim_model = 3, three words can have relationships with one another.\n",
        "\n"
      ],
      "metadata": {
        "id": "bg7d-nsfxHgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing above example\n",
        "test_embedding = nn.Embedding(10,3)\n",
        "print(test_embedding.weight)\n",
        "print(test_embedding.weight.shape)\n",
        "\n",
        "test_input = torch.tensor([[1,2,3],[1,1,1]])\n",
        "print(test_embedding(test_input))\n",
        "print(test_embedding(test_input).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iqSoMdt0CxD",
        "outputId": "6334fd85-54d8-4672-a8c9-528f7471506c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.4508,  0.6154,  0.2267],\n",
            "        [-0.5689, -0.3536, -0.5198],\n",
            "        [ 0.2794, -0.3265,  0.3787],\n",
            "        [-0.9912, -0.2752,  2.4077],\n",
            "        [ 1.1389,  0.7706, -1.3733],\n",
            "        [ 0.9573,  1.8728,  0.2682],\n",
            "        [-0.9222,  0.2009,  1.5517],\n",
            "        [-0.6816,  1.1102, -1.9568],\n",
            "        [-0.7798,  0.0606,  0.6875],\n",
            "        [ 0.5475, -1.3501, -1.2198]], requires_grad=True)\n",
            "torch.Size([10, 3])\n",
            "tensor([[[-0.5689, -0.3536, -0.5198],\n",
            "         [ 0.2794, -0.3265,  0.3787],\n",
            "         [-0.9912, -0.2752,  2.4077]],\n",
            "\n",
            "        [[-0.5689, -0.3536, -0.5198],\n",
            "         [-0.5689, -0.3536, -0.5198],\n",
            "         [-0.5689, -0.3536, -0.5198]]], grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  \"\"\" This module creates an embedding matrix of dimensions (vocab_size, d_model) \"\"\"\n",
        "\n",
        "  #IMP - swapping d_model and vocab_size input the other way round compared to video\n",
        "  def __init__(self,  vocab_size: int, d_model: int):\n",
        "    #the super method inherits properties from another class\n",
        "    super().__init__()\n",
        "    #d_model is the dimension of the model\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    #this step creates an embedding matrix of dimensions (vocab_size, d_model)\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    #you can check out the embedding matrix by running \"\"nn.Embedding(4,3).weight\"\"\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" multiplying embedding by square root of the model dimension, per paper\"\"\"\n",
        "    #x is the input tokens\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "ozAzzV3mm5Ss"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Positional Encoding**"
      ],
      "metadata": {
        "id": "VR8-yxlD4qpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of positional encoding is to add some perturberation to the input sentences in a way that they keep their order. This is done using the sin and cos functions.\n",
        "\n",
        "![](https://raw.githubusercontent.com/mbilgrami/images/main/transformer_images/PositionalEncoding.png)"
      ],
      "metadata": {
        "id": "4giy-oTh-aBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  \"\"\" This class creates a positional encoding matrix of dimensions (seq_len, d_model) \"\"\"\n",
        "  #IMP - this will work as long as the constraint d_model is even is met\n",
        "\n",
        "  #having to change shape around to make consistent with previous part\n",
        "  # d_model is the dimension of the model, same as in the embeddings.\n",
        "  #dropout = Dropout layer is added to prevent the model from overfitting\n",
        "  #seq_len = maximum length of the sentence\n",
        "  def __init__(self, seq_len: int, d_model: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    #building matrix of shape seq_len, d_model.\n",
        "    #the vector has to be of d_model size (to capture each word), but we need seq_len of them to ensure that we can capture all sentences.\n",
        "    #created a tensor of zeros of the set up ([0,0,... d_model], [0,0,...d_model]... seq_len times/rows).\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "    #creating a vector of shape (seq_len, 1)\n",
        "    #this looks like [[0],[1],[2]...[seq_len]].\n",
        "    position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1)\n",
        "\n",
        "    #div_term is a vector of shape (d_model, 1)\n",
        "    #torch.arange(0, d_model, 2) creates a tensor of [0,1,2,...,d_model], and then skips over every alternate element.\n",
        "    #this makes it [0,2,4,... d_model]\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model))\n",
        "\n",
        "    #note that (position * div_term) creates a tensor of size (seq_len, div_term/2).\n",
        "    #The shape goes funny with odd d_model and is incompatible with the pe slices we do further down.\n",
        "\n",
        "    #apply sin to even position\n",
        "    pe[:, 0::2] = torch.sin(position * div_term) #this replaces every 2nd column of the pe with this sin values starting from 0th column\n",
        "\n",
        "    #apply cos to odd position\n",
        "    pe[:, 1::2] = torch.cos(position * div_term) #this replaces every 2nd column of the pe with this cos values starting from 1st column\n",
        "\n",
        "    #accounting for the batch dimensions, as we will get a batch of sentences as input, not just one.\n",
        "    pe = pe.unsqueeze(0) #tensor of shape (1, seq_len, d_model)\n",
        "    #want this tensor to be saved when we save the file of the model\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" adding positional encoding to every word inside a batch of sentences.\n",
        "    The batch comes in the shape (batch_size, num_words, d_model)\n",
        "     \"\"\"\n",
        "\n",
        "    #remember that pe has shape (1, seq_len, d_model). We want to add pe to the word to help order it.\n",
        "    #pe is ordered as [1 = batch_size, seq_len, d_model].\n",
        "    #As seq_len is the max words in a sentence, it will always be greater than x.shape[1] (which is the number of words in sentence)\n",
        "    #adding x to itself means that the tensor will keep getting appended with values in the sentence batch.\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) #requires_grad_(False) means training wont update weights - this isnt needed\n",
        "    return self.dropout(x) #dropout means some of the numbers will become 0 - helps with trainning"
      ],
      "metadata": {
        "id": "gyfG0YF0p5V_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer Normalisation**\n",
        "\n",
        "Let's say we have a batch of 3 sentences, and each sentence is made up of its own features (words, etc).\n",
        "\n",
        "Layer normalisation means that for each item (sentence), we calculate the mean and the variance of each item (sentence) independently from the other items in the batch.\n",
        "\n",
        "$$\n",
        "\\hat X_j = \\frac{X_j - \\mu_j}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "where\n",
        "* $\\hat X_j$ = estimated value of item\n",
        "* $\\mu_j$ = mean of item\n",
        "* $\\sigma^2$ = variance of item\n",
        "* $\\epsilon$ : Needed to ensure $\\hat X_j$ doesnt get very big in case $\\sigma^2$ gets too small. Undesirable because the CPU/GPU can't capture numbers that are very large\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Each item will get its own $\\hat X_j$\n",
        "\n",
        "We also add two parameters, usually called gamma (multiplicative) and beta (additive) that introduce some fluctuations in the data, because maybe having all values between 0 and 1 may be too restrictive for the network. The network will learn to tune these two parameters and introduce fluctuations when necessary."
      ],
      "metadata": {
        "id": "I15dYduSPsUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalisation(nn.Module):\n",
        "  \"\"\" This class creates a layer normalisation layer. Helps keep network in check\"\"\"\n",
        "\n",
        "  def __init__(self, eps: float = 10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1)) #creates a tensor [1] - multiplied\n",
        "    self.bias = nn.Parameter(torch.zeros(1)) #creates a tensor [0] - added\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim = -1, keepdim = True)\n",
        "    std = x.std(dim = -1, keepdim = True)\n",
        "    return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
      ],
      "metadata": {
        "id": "DAjsOiGMG-Tu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feedforward**\n",
        "\n",
        "Fully connected layer used in the encoder and the decoder. It's covered in the paper in the following section:\n",
        "\n",
        "&nbsp;\n",
        "\"\"\n",
        "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
        "connected feed-forward network, which is applied to each position separately and identically. This\n",
        "consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "> FFN(x) = max(0, *x*$W_1$ + $b_1$)$W_2$ + $b_2$\n",
        "\n",
        "While the linear transformations are the same across different positions, they use different parameters\n",
        "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
        "The dimensionality of input and output is $d_{model}$ = 512, and the inner-layer has dimensionality\n",
        "$d_{ff}$ = 2048\n",
        "\"\"\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "\n",
        "These are basically two matrices: $W_1$ and $W_2$ that are being multiplied by the x (input??) one after another with a ReLu in between with a bias $b_1$."
      ],
      "metadata": {
        "id": "ek3ybPevXt7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout:float) -> None:\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff) #creating W1 and b1 (Linear automatically defaults to bias = True)\n",
        "    self.dropout = nn.Dropout(dropout) #creating dropout layer\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model) #creating W2 and b2 (Linear automatically defaults to bias = True)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    \"\"\"takes input x which is a batch of sentences with Tensor (batch_size, seq_len, d_model)\n",
        "    - linear1 will convert input batch to (batch_size, seq_len, d_model)\n",
        "    - linear2 will convert back to (batch_size, seq_len, d_model) ... note that the order of d_ff and d_model is reversed between linear1 and linear2 to help with tensor multiplication\n",
        "    \"\"\"\n",
        "\n",
        "    hidden = self.linear_1(x) #multiplying input by linear 1 to give (batch_size, seq_len, d_ff)\n",
        "    hidden = torch.relu(hidden) #applying relu function\n",
        "    hidden = self.dropout(hidden) #applying dropout\n",
        "    output = self.linear_2(hidden) #multiplying hidden by linear 2 to give (batch_size, seq_len, d_model)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xuPuacPspdCC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention**\n",
        "\n",
        "Attention takes the input of the encoder and uses it three times: Query, Key and Vector... kind of like a duplication of the input three times.\n",
        "\n",
        "We take an input of (sequence length, d_model) and then transform it into three matrices: Q, K, V. These are exactly the same as the input for the encoder part (left box in initial architecture). So they have the same dimensions as the input matrix.\n",
        "\n",
        "We split these Q, K, V matrices into h smaller matrices, and we do this split across the d_model dimension... meaning each head will have access to the full sentence, but a different part of an embedding of each word. 'h' represents a head in the multi-headed attention.\n",
        "\n",
        "We apply attention to the smaller h matrices using the formula\n",
        "\n",
        "$$\n",
        "Attention(Q,K,V) = softmax(\\frac{Q{K^T}}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "\n",
        "> $head_i$ = Attention($Q{W_i^Q},K{W_i^K},V{W_i}^V)$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Then we combine the smaller h matrices back into the larger matrix:\n",
        "\n",
        "> Multihead(Q,K,V) = $Concat(head_1,...,head_i){W^0}$\n",
        "\n",
        "\n",
        "where:\n",
        "* H is a concatenation of h with dimensions (seq_len, h * $d_V$)\n",
        "* ${W^O}$ has dimensions (seq_len, h * $d_{V^T}$)\n",
        "* Multihead(Q,K,V) has dimensions (seq_len, dim_model), which is a multiplication of H and ${W^O}$\n",
        "* $d_k$ denotes $\\frac{d_{model}}{h}$ where h is the number of heads $d_{model}$ will be divided into.  \n",
        "* Note that although $d_V$ implies the denotion of the vector matrix, it's exactly the same dimension as the key matrix  matrix K (i.e. $d_K$) and the query (i.e. $d_Q$).\n",
        "\n",
        "The representation is for one sentence, but we should also remember that there is an additional dimension for the batch size.\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/mbilgrami/images/main/transformer_images/AttentionLayout%20-%20Umar%20Jamil.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "te-u8VKOQEUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, h:int, dropout:float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "\n",
        "    #we need to divide d_model into equal heads, so d_model needs to be divisible by h\n",
        "    assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    #getting value d_l\n",
        "    self.d_k = d_model // h #note that // means that it will divide and floor to nearest round number\n",
        "\n",
        "    #creating w matrix to apply a linear transformation, so that Q/K/V {dim (seq_len, d_model)} x W {dim (d_model, d_model)} = Q'/W'/K' {dim (seq_len, d_model)}\n",
        "    self.w_q = nn.Linear(d_model, d_model) #creating wq\n",
        "    self.w_k = nn.Linear(d_model, d_model) #creating wk\n",
        "    self.w_v = nn.Linear(d_model, d_model) #creating wv\n",
        "\n",
        "    #creating w_o matrix, which is multiplied to the concats of the h matrices.\n",
        "    #Note that d_v * h = d_model, so we've input the dimensions as (d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model) #creating wo\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout) #creating dropout layer\n",
        "\n",
        "\n",
        "  #A note on mask: when we implement softmax(Q.K_transposed / sqrt(d_k)), it creates a (seq_len, seq_len) matrix.\n",
        "  #Remember that seq_len is the maximum length of the sentence\n",
        "  #if we don't want certain words in the sentences to not interact with other words, we put a mask on them by putting a really small value.\n",
        "  #The softmax makes those the values for the attention between those two words 0.\n",
        "\n",
        "  @staticmethod #creates a function without needing an instance of the class\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "    d_k = query.shape[-1] #d_k is the last dimension of the query matrix Qi (split into different heads) implemented in the forward function below\n",
        "\n",
        "    #calculating attention scores. See Attention (Q,K,V) formula in image above.\n",
        "    #the @ sign represents matrix multiplication in pytorch.\n",
        "    #the transpose transposes the key matrix's last 2 dimensions to change from (seq_len, d_k) to (d_k, seq_len)\n",
        "    #multiplication changes (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) #will output (batch, h, seq_len, seq_len)\n",
        "\n",
        "    #we want to apply mask before softmax to hide some words\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill(mask == 0, -1e9) #replace all values of cases where mask = 0 with a really high negative number. This will become 0 post softmax\n",
        "    attention_scores = attention_scores.softmax(dim = -1) #(batch, h, seq_len, seq_len)\n",
        "\n",
        "    #applying dropout\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    #returning multiplication of above and value matrix (per formula) as a tuple for the model\n",
        "    #also returning the raw attention scores for visualising what is the score given by the model for the interaction\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    query =self.w_q(q) #this gives Q' from the image. Goes from (batch, seq_len, d_model) ---> (batch, seq_len, d_model), as (batch, seq_len, d_model) x (batch, seq_len, seq_len) = (batch, seq_len, d_model)\n",
        "    key = self.w_k(k) #this gives K' from the image. Goes from (batch, seq_len, d_model) ---> (batch, seq_len, d_model)\n",
        "    value = self.w_v(v) #this gives V' from the image. Goes from (batch, seq_len, d_model) ---> (batch, seq_len, d_model)\n",
        "\n",
        "    #we want to divide the Query, Key and Value matrices in smaller matrices using h heads\n",
        "    #the view method of pytorch divides a matrix into different dimensions.\n",
        "    #we want to split the Q,K,V matrices into the following dimensions: (batch {query.shape[0]}, seq_len {query.shape[1]}, h {self.h}, d_k {self.d_dk}), where d_k = d_model / h\n",
        "    #we transpose the matrix to get it in the order (batch, h, seq_len, d_k).\n",
        "    #We transpose because we want each head to watch the seq_len and d_k. This means it will see the full sentence, but a smaller part of the embedding\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    #CONCATENATING INDIVIDUAL HEADS TOGETHER\n",
        "    #Takes in Attention(Q,K,V) of dim(batch, h, seq_len, d_k)\n",
        "    #(batch, h, seq_len, d_k) --> batch(batch, seq_len, h, d_k) --> (Batch, seq_len, d_model)\n",
        "    #the transpose applies the first transformation. The contiguous applies the second one.\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k) #where self.h * self.d_k = d_model\n",
        "\n",
        "    #(batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "    return self.w_o(x)\n"
      ],
      "metadata": {
        "id": "p2MiDOx5I5sq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Residual Connection\n",
        "\n",
        "This is shown as **Add & Norm** on the diagram. We add the input (after PE) to the output of the multi-head attention block."
      ],
      "metadata": {
        "id": "x-WF-gSOqiVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalisation()    #previously defined\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    \"\"\" x: input (after PE)\n",
        "        sublayer: previous layer after multi-head attention\"\"\"\n",
        "    y = self.norm(x) #applying normalisation to the input x. Still has (batch_size, seq_len, d_model)\n",
        "    y = sublayer(y) #applying the sublayer to the normalised input x. Will also end up with (batch_size, seq_len, d_model)... see MultiHeadAttention\n",
        "\n",
        "    return x + self.dropout(y) #adding dropout with normalised sublayer, and adding it to input: (batch_size, seq_len, d_model)\n",
        ""
      ],
      "metadata": {
        "id": "DbyVBFU_I6kx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block\n",
        "\n",
        "We put together all of the above parts together in the encoder block. Refer to encoder block in the chart above."
      ],
      "metadata": {
        "id": "MVzaNKcr0a0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "\n",
        "    #adding a couple of residual connectors in, per encoder architecture.\n",
        "    #the nn.ModuleList is a way to organise modules in pytorch. We will be using 2 residual connection modules\n",
        "    # the '_' in 'for _ in range(2)' is used instead of i when we don't really care about the value of i\n",
        "    self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    \"\"\" x: input (after PE)\n",
        "        src_mask: mask for the input of the encoder to hide the interaction of the padding words with other words \"\"\"\n",
        "    #residual_connection[0] calls the first residual connection, while residual_connection[1] calls the second one in the block\n",
        "    #remember that the ResidualConnection layer takes in the input x and the sublayer.\n",
        "    #the sublayer is the multiheadattention block (see function), with the query key and value matrices as copies of the input matrix x, along with a mask.\n",
        "    x = self.residual_connection[0](x, lambda x: self.attention_block(x, x, x, src_mask))\n",
        "    x = self.residual_connection[1](x, self.feed_forward_block) #this layer takes in the output of the add&norm, and adds it to the output of the feedforward\n",
        "    return x\n",
        "\n",
        "#We can have upto N encoder objects according to the paper. stacking Encoder blocks together captures more depth. Paper recommends 6.\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layers: nn.ModuleList) #many layers, applied one after another iteratively. So the output of one layer goes into another\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalisation()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)\n"
      ],
      "metadata": {
        "id": "gjyvyPgw0hSW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scratchpad"
      ],
      "metadata": {
        "id": "vnu7vXrqI7Vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding Implementation"
      ],
      "metadata": {
        "id": "VvjlmtjAI_p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a batch of 2 sentences - tensor dims are (sentences in batch, words in sentences, dim_model)\n",
        "x = torch.tensor([[[1,2,3,4],[4,5,6,7]],\n",
        " [[3,2,3,4],[4,5,6,7]]])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENj_2Z-EPeNO",
        "outputId": "a46b773e-77c5-430d-8f1b-0b24a1db3b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 2, 3, 4],\n",
            "         [4, 5, 6, 7]],\n",
            "\n",
            "        [[3, 2, 3, 4],\n",
            "         [4, 5, 6, 7]]])\n",
            "torch.Size([2, 2, 4])\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = PositionalEncoding(4,4,0.1).forward(x)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqzUy_bJPfTM",
        "outputId": "c69bb688-fc45-4a46-b95b-b2b7d7fd6d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.1111, 3.3333, 3.3333, 5.5556],\n",
              "         [5.3794, 6.1559, 6.6778, 8.8888]],\n",
              "\n",
              "        [[3.3333, 3.3333, 3.3333, 5.5556],\n",
              "         [5.3794, 6.1559, 6.6778, 8.8888]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "2PD6ql4Velvh",
        "outputId": "6ab153f4-a353-4e54-959e-f221edc672a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "masked_fill() received an invalid combination of arguments - got (bool, float), but expected one of:\n * (Tensor mask, Tensor value)\n      didn't match because some of the arguments have invalid types: (!bool!, !float!)\n * (Tensor mask, Number value)\n      didn't match because some of the arguments have invalid types: (!bool!, !float!)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-828f5cd77108>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mMultiHeadAttentionBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-bfe8494f0c23>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttentionBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m#CONCATENATING INDIVIDUAL HEADS TOGETHER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-bfe8494f0c23>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#we want to apply mask before softmax to hide some words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#replace all values of cases where mask = 0 with a really high negative number. This will become 0 post softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch, h, seq_len, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: masked_fill() received an invalid combination of arguments - got (bool, float), but expected one of:\n * (Tensor mask, Tensor value)\n      didn't match because some of the arguments have invalid types: (!bool!, !float!)\n * (Tensor mask, Number value)\n      didn't match because some of the arguments have invalid types: (!bool!, !float!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 4\n",
        "d_model = 4\n",
        "\n",
        "pe = torch.zeros(seq_len, d_model)\n",
        "print(pe)\n",
        "\n",
        "position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1)\n",
        "print(position)\n",
        "\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model))\n",
        "print(\"div_term is:\")\n",
        "print(div_term)\n",
        "\n",
        "print(\"sin of position * div_term is:\")\n",
        "print(torch.sin(position * div_term))\n",
        "\n",
        "pe[:, 0::2] = torch.sin(position * div_term) #0::2 means start from 0 and go forward by 2 (every alternate)\n",
        "\n",
        "print(\"updated pe with sin is\")\n",
        "print(pe)\n",
        "\n",
        "print(\"cos of position * div_term is:\")\n",
        "print(torch.cos(position * div_term))\n",
        "\n",
        "print(\"base pe is:\")\n",
        "print(pe)\n",
        "print(\"extracted value of pe to be replaced is:\")\n",
        "print(pe[:,1::2])\n",
        "\n",
        "print(\"cos of pe is\")\n",
        "pe[:, 1::2] = torch.cos(position * div_term) #0::2 means start from 0 and go forward by 2 (every alternate)\n",
        "print(pe[:, 1::2])\n",
        "\n",
        "print(\"new pe is:\")\n",
        "print(pe)\n",
        "\n",
        "print(\"unsqueezed pe is:\")\n",
        "pe = pe.unsqueeze(0) #tensor of shape (1, seq_len, d_model)\n",
        "print(pe)\n",
        "\n",
        "print(\"x input is\")\n",
        "print(x)\n",
        "\n",
        "print(\"pe slice is\")\n",
        "print(pe[:, :x.shape[1], :]) #x.shape[1] is the number of words in the sentence\n",
        "\n",
        "print(\"final sentence is\")\n",
        "print(x + pe[:, :x.shape[1], :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREAEPbHHgVU",
        "outputId": "6957578e-5d69-4022-f3d8-55ce59f6e79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "div_term is:\n",
            "tensor([1.0000, 0.0100])\n",
            "sin of position * div_term is:\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [0.8415, 0.0100],\n",
            "        [0.9093, 0.0200],\n",
            "        [0.1411, 0.0300]])\n",
            "updated pe with sin is\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.8415, 0.0000, 0.0100, 0.0000],\n",
            "        [0.9093, 0.0000, 0.0200, 0.0000],\n",
            "        [0.1411, 0.0000, 0.0300, 0.0000]])\n",
            "cos of position * div_term is:\n",
            "tensor([[ 1.0000,  1.0000],\n",
            "        [ 0.5403,  0.9999],\n",
            "        [-0.4161,  0.9998],\n",
            "        [-0.9900,  0.9996]])\n",
            "base pe is:\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.8415, 0.0000, 0.0100, 0.0000],\n",
            "        [0.9093, 0.0000, 0.0200, 0.0000],\n",
            "        [0.1411, 0.0000, 0.0300, 0.0000]])\n",
            "extracted value of pe to be replaced is:\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "cos of pe is\n",
            "tensor([[ 1.0000,  1.0000],\n",
            "        [ 0.5403,  0.9999],\n",
            "        [-0.4161,  0.9998],\n",
            "        [-0.9900,  0.9996]])\n",
            "new pe is:\n",
            "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
            "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
            "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
            "        [ 0.1411, -0.9900,  0.0300,  0.9996]])\n",
            "unsqueezed pe is:\n",
            "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
            "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
            "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
            "         [ 0.1411, -0.9900,  0.0300,  0.9996]]])\n",
            "x input is\n",
            "tensor([[[1, 2, 3, 4],\n",
            "         [4, 5, 6, 7]],\n",
            "\n",
            "        [[3, 2, 3, 4],\n",
            "         [4, 5, 6, 7]]])\n",
            "pe slice is\n",
            "tensor([[[0.0000, 1.0000, 0.0000, 1.0000],\n",
            "         [0.8415, 0.5403, 0.0100, 0.9999]]])\n",
            "final sentence is\n",
            "tensor([[[1.0000, 3.0000, 3.0000, 5.0000],\n",
            "         [4.8415, 5.5403, 6.0100, 7.9999]],\n",
            "\n",
            "        [[3.0000, 3.0000, 3.0000, 5.0000],\n",
            "         [4.8415, 5.5403, 6.0100, 7.9999]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#where does the d_model get split?"
      ],
      "metadata": {
        "id": "EtjYTidLLHnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd3976b-2965-4f02-f627-6d6eac1374bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3L8yIF41fUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}